{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb1acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, current_date\n",
    "\n",
    "# Initialize SparkSession\n",
    "def get_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"DataQualityChecks\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Function to validate schema\n",
    "def validate_schema(df, expected_schema):\n",
    "    actual_schema = df.dtypes\n",
    "    for column, expected_type in expected_schema:\n",
    "        assert (column, expected_type) in actual_schema, f\"Column {column} with type {expected_type} is missing or mismatched.\"\n",
    "    print(\"Schema validation passed.\")\n",
    "\n",
    "# Function to check null values in critical columns\n",
    "def check_nulls(df, columns):\n",
    "    for col_name in columns:\n",
    "        null_count = df.filter(df[col_name].isNull()).count()\n",
    "        assert null_count == 0, f\"Null values found in column: {col_name}\"\n",
    "    print(\"Null check passed for all specified columns.\")\n",
    "\n",
    "# Function to validate data types\n",
    "def validate_data_types(df, column_data_types):\n",
    "    for column, expected_type in column_data_types.items():\n",
    "        actual_type = dict(df.dtypes)[column]\n",
    "        assert actual_type == expected_type, f\"Column {column} has type {actual_type}, expected {expected_type}.\"\n",
    "    print(\"Data type validation passed.\")\n",
    "\n",
    "# Function to check for duplicates in critical columns\n",
    "def check_duplicates(df, column):\n",
    "    duplicate_count = df.groupBy(column).count().filter(\"count > 1\").count()\n",
    "    assert duplicate_count == 0, f\"Duplicates found in column: {column}\"\n",
    "    print(f\"No duplicates found in column: {column}\")\n",
    "\n",
    "# Function to validate start date (no future dates)\n",
    "def validate_start_date(df):\n",
    "    future_count = df.filter(to_date(df[\"Entity Start Date\"], \"yyyy-MM-dd\") > current_date()).count()\n",
    "    assert future_count == 0, \"Some companies have a start date in the future.\"\n",
    "    print(\"Start date validation passed.\")\n",
    "\n",
    "# Test function for schema validation\n",
    "def test_schema_validation(spark):\n",
    "    # Load test data\n",
    "    abn_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/abn_data.csv\")\n",
    "    expected_schema = [(\"ABN\", \"string\"), (\"Entity Name\", \"string\"), (\"Entity Type\", \"string\")]\n",
    "    validate_schema(abn_df, expected_schema)\n",
    "\n",
    "# Test function for null check\n",
    "def test_null_check(spark):\n",
    "    abn_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/abn_data.csv\")\n",
    "    critical_columns = [\"ABN\", \"Entity Name\", \"Entity Type\", \"Entity Start Date\"]\n",
    "    check_nulls(abn_df, critical_columns)\n",
    "\n",
    "# Test function for checking duplicates\n",
    "def test_duplicates(spark):\n",
    "    abn_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/abn_data.csv\")\n",
    "    check_duplicates(abn_df, \"ABN\")\n",
    "\n",
    "# Test function for start date validation\n",
    "def test_start_date(spark):\n",
    "    abn_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/abn_data.csv\")\n",
    "    validate_start_date(abn_df)\n",
    "\n",
    "\n",
    "# Main pipeline execution function\n",
    "def run_pipeline(spark):\n",
    "    # Load the datasets (mock paths, change as needed)\n",
    "    abn_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/abn_data.csv\")\n",
    "    common_crawl_df = spark.read.option(\"header\", \"true\").csv(\"/path/to/common_crawl_data.csv\")\n",
    "    \n",
    "    # Validate schemas and data quality\n",
    "    expected_schema = [(\"ABN\", \"string\"), (\"Entity Name\", \"string\"), (\"Entity Type\", \"string\"), (\"Entity Start Date\", \"string\")]\n",
    "    validate_schema(abn_df, expected_schema)\n",
    "    check_nulls(abn_df, [\"ABN\", \"Entity Name\", \"Entity Type\", \"Entity Start Date\"])\n",
    "    check_duplicates(abn_df, \"ABN\")\n",
    "    validate_start_date(abn_df)\n",
    "    \n",
    "# Running the pipeline (for testing)\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark_session()\n",
    "    run_pipeline(spark)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
